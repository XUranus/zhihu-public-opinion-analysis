{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import jieba\n",
    "import re\n",
    "import csv\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用tensorflow的keras接口来建模\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding\n",
    "embedding = KeyedVectors.load_word2vec_format('sgns.zhihu.bigram-char', binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding维度300\n",
    "embedding_dim = embedding['中国'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5562877"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.similarity('中国','美国')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.vocab['中国'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('中国人', 0.5616261959075928),\n",
       " ('美国', 0.5562876462936401),\n",
       " ('我国', 0.5315867066383362),\n",
       " ('全中国', 0.5306392908096313),\n",
       " ('中国茶', 0.5249154567718506),\n",
       " ('中国海', 0.5224688053131104),\n",
       " ('中国武协', 0.5200954079627991),\n",
       " ('外国', 0.5197731256484985),\n",
       " ('中国篮球', 0.511111319065094),\n",
       " ('日本', 0.5098267793655396)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.most_similar(positive=['中国'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361744 text examples in trainset\n"
     ]
    }
   ],
   "source": [
    "#原始数据准备\n",
    "train_text_orig = []\n",
    "train_target = []\n",
    "\n",
    "csv_orig = csv.reader(open('simplifyweibo_4_moods.csv'))\n",
    "#'喜悦', 1: '愤怒', 2: '厌恶', 3: '低落\n",
    "#1 表示正向评论，0 表示负csv_orig向评论\n",
    "next(csv_orig, None)\n",
    "for line in csv_orig:\n",
    "    train_text_orig.append(line[1])\n",
    "    train_target.append(line[0])\n",
    "\n",
    "train_target = np.array(train_target).astype('int')\n",
    "print('%d text examples in trainset' %len(train_text_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to onehot\n",
    "train_target = to_categorical(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清晰数据\n",
    "def clean_text(text):\n",
    "    text = re.sub(\"<[^>]+>\", \"\", text)\n",
    "    text = text.replace(\"&nbsp;\", \"\")\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）-]+\", \"\", text)\n",
    "    text = re.sub(\"[^0-9A-Za-z\\u4e00-\\u9fa5]\", \"\", text)\n",
    "    text = re.sub( \"\\\\(.*?\\\\)|\\\\{.*?}|\\\\[.*?]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    words = [w for w in jieba.cut(text)]\n",
    "    embedding_vectors = []\n",
    "    for idx, word in enumerate(words):\n",
    "        try:\n",
    "            embedding_vectors.append(embedding.vocab[word].index)\n",
    "        except KeyError:\n",
    "            embedding_vectors.append(0)\n",
    "    return embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.944 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.944 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#tokenize\n",
    "train_tokens = []\n",
    "for text in  train_text_orig:\n",
    "    pure_text = clean_text(text)\n",
    "    tokens = tokenize_text(pure_text)\n",
    "    train_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.77766320934141"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#平均token数\n",
    "num_tokens = [len(tokens) for tokens in train_tokens]\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9301025034278385"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为80时，大约 93%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "max_tokens = 80\n",
    "np.sum( np.array(num_tokens) < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'回复 了买房子送瓷砖呗昨晚上经过 看到的一个立柱价格应该 可是看了半天也没看明白诉求点是什么'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + embedding.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text\n",
    "\n",
    "reverse_tokens(train_tokens[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259753"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取100000/259753个词\n",
    "num_words = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,  3236,     1, 29302,\n",
       "         102,   170, 71942,   372,   223,   939,   660,  3185, 36693,\n",
       "           1,  6409, 48197, 88738,   477, 14927,  3625,  1122,  1342,\n",
       "         178,    65,     1,  1765,   648,   223,     1,  3454,  3185,\n",
       "        2387,   110,    14,     0,    32,    41,     1, 23948,     0,\n",
       "         102,  2387,  1069,  1866,  1315, 10110,     1,  4403],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens, padding='pre', truncating='pre')\n",
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[ train_pad>=num_words ] = 0\n",
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "train_pad[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(259753, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用259753个词\n",
    "num_words = 259753\n",
    "embedding_dim = 300\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = embedding[embedding.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n",
    "np.array(embedding_matrix).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 80, 300)           77925900  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 80, 128)           186880    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 16)                9280      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 78,122,128\n",
      "Trainable params: 196,228\n",
      "Non-trainable params: 77,925,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                   embedding_dim,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length = max_tokens,\n",
    "                   trainable = False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 293012 samples, validate on 32557 samples\n",
      "Epoch 1/20\n",
      "293012/293012 [==============================] - 428s 1ms/step - loss: 0.4376 - accuracy: 0.8129 - val_loss: 0.4236 - val_accuracy: 0.8197\n",
      "Epoch 2/20\n",
      "293012/293012 [==============================] - 282s 962us/step - loss: 0.4174 - accuracy: 0.8218 - val_loss: 0.4162 - val_accuracy: 0.8226\n",
      "Epoch 3/20\n",
      "293012/293012 [==============================] - 363s 1ms/step - loss: 0.4050 - accuracy: 0.8270 - val_loss: 0.4102 - val_accuracy: 0.8252\n",
      "Epoch 4/20\n",
      "293012/293012 [==============================] - 272s 929us/step - loss: 0.3913 - accuracy: 0.8324 - val_loss: 0.4058 - val_accuracy: 0.8277\n",
      "Epoch 5/20\n",
      "293012/293012 [==============================] - 273s 932us/step - loss: 0.3760 - accuracy: 0.8384 - val_loss: 0.4056 - val_accuracy: 0.8285\n",
      "Epoch 6/20\n",
      "293012/293012 [==============================] - 670s 2ms/step - loss: 0.3603 - accuracy: 0.8438 - val_loss: 0.4066 - val_accuracy: 0.8283\n",
      "Epoch 7/20\n",
      "293012/293012 [==============================] - 305s 1ms/step - loss: 0.3444 - accuracy: 0.8493 - val_loss: 0.4076 - val_accuracy: 0.8293\n",
      "Epoch 8/20\n",
      "293012/293012 [==============================] - 280s 955us/step - loss: 0.3301 - accuracy: 0.8544 - val_loss: 0.4076 - val_accuracy: 0.8306\n",
      "Epoch 9/20\n",
      "293012/293012 [==============================] - 277s 944us/step - loss: 0.3166 - accuracy: 0.8587 - val_loss: 0.4088 - val_accuracy: 0.8311\n",
      "Epoch 10/20\n",
      "293012/293012 [==============================] - 278s 950us/step - loss: 0.3042 - accuracy: 0.8631 - val_loss: 0.4153 - val_accuracy: 0.8325\n",
      "Epoch 11/20\n",
      "293012/293012 [==============================] - 279s 953us/step - loss: 0.2933 - accuracy: 0.8665 - val_loss: 0.4168 - val_accuracy: 0.8323\n",
      "Epoch 12/20\n",
      "293012/293012 [==============================] - 279s 952us/step - loss: 0.2832 - accuracy: 0.8695 - val_loss: 0.4280 - val_accuracy: 0.8310\n",
      "Epoch 13/20\n",
      "293012/293012 [==============================] - 278s 948us/step - loss: 0.2742 - accuracy: 0.8726 - val_loss: 0.4322 - val_accuracy: 0.8337\n",
      "Epoch 14/20\n",
      "293012/293012 [==============================] - 278s 949us/step - loss: 0.2660 - accuracy: 0.8751 - val_loss: 0.4444 - val_accuracy: 0.8295\n",
      "Epoch 15/20\n",
      "293012/293012 [==============================] - 289s 985us/step - loss: 0.2585 - accuracy: 0.8775 - val_loss: 0.4491 - val_accuracy: 0.8319\n",
      "Epoch 16/20\n",
      "293012/293012 [==============================] - 277s 946us/step - loss: 0.2519 - accuracy: 0.8793 - val_loss: 0.4510 - val_accuracy: 0.8321\n",
      "Epoch 17/20\n",
      "293012/293012 [==============================] - 278s 949us/step - loss: 0.2460 - accuracy: 0.8816 - val_loss: 0.4558 - val_accuracy: 0.8345\n",
      "Epoch 18/20\n",
      "293012/293012 [==============================] - 282s 962us/step - loss: 0.2408 - accuracy: 0.8831 - val_loss: 0.4692 - val_accuracy: 0.8331\n",
      "Epoch 19/20\n",
      "293012/293012 [==============================] - 279s 951us/step - loss: 0.2350 - accuracy: 0.8853 - val_loss: 0.4768 - val_accuracy: 0.8325\n",
      "Epoch 20/20\n",
      "293012/293012 [==============================] - 282s 961us/step - loss: 0.2307 - accuracy: 0.8864 - val_loss: 0.4823 - val_accuracy: 0.8335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f28fc053cd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=20,\n",
    "          batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36175/36175 [==============================] - 34s 933us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.476165550299653, 0.8339530229568481]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "#95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('senti4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    pure_text = clean_text(text)\n",
    "    tokens = tokenize_text(pure_text)\n",
    "    tokens_pad = pad_sequences([tokens], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(tokens_pad)\n",
    "    result_text = ['喜悦','愤怒', '厌恶','低落']\n",
    "    print(result)\n",
    "    print(result_text[np.argmax(result)])\n",
    "    return np.argmax(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21768999 0.31354317 0.23442587 0.23434097]]\n",
      "愤怒\n",
      "[[0.87876415 0.10020134 0.01076278 0.01027178]]\n",
      "喜悦\n",
      "[[9.8751849e-01 1.1235228e-02 6.2838796e-04 6.1791926e-04]]\n",
      "喜悦\n",
      "[[0.0090515  0.19992171 0.3952723  0.39575452]]\n",
      "低落\n",
      "[[0.7592637  0.14819947 0.04622012 0.04631674]]\n",
      "喜悦\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"品控不好，还没到一个月就坏了\")\n",
    "predict_sentiment(\"品控不错，挺好的\")\n",
    "predict_sentiment(\"太开心了\")\n",
    "predict_sentiment(\"难受啊\")\n",
    "predict_sentiment(\"谢天牛逼啊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = [np.argmax(arr) for arr in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22114\n",
      "36175\n",
      "0.6113061506565307\n"
     ]
    }
   ],
   "source": [
    "ss = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i]==np.argmax(y_test[i])):\n",
    "        ss+=1\n",
    "print(ss)\n",
    "print(len(y_pred))\n",
    "print(ss/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-62-1bd56390c2d0>:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  misclassified = np.where( y_pred != y_test )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0]),)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified = np.where( y_pred != y_test )\n",
    "misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in misclassified:\n",
    "    print(reverse_tokens(X_test[idx]))\n",
    "    print('预测的分类', y_pred[idx])\n",
    "    print('实际的分类', y_actual[idx])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y_pred==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_tokens(X_test[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment('感谢大力支持赞家中常备红星二锅头拥有时刻好心情的说哈哈')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment('小米业界良心')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 80, 300)           77925900  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 80, 128)           186880    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 16)                9280      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 78,122,077\n",
      "Trainable params: 196,177\n",
      "Non-trainable params: 77,925,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#from keras.models import load_model\n",
    "from keras.models import load_model\n",
    "model_loaded = load_model('senti3.h5')\n",
    "model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.616 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.616 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d28ff3596a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpure_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpure_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m tokens_pad = pad_sequences([tokens], maxlen=max_tokens,\n\u001b[0m\u001b[1;32m      5\u001b[0m                        padding='pre', truncating='pre')\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 预测\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "text = '小米实在是太厉害了'\n",
    "pure_text = clean_text(text)\n",
    "tokens = tokenize_text(pure_text)\n",
    "tokens_pad = pad_sequences([tokens], maxlen=max_tokens,\n",
    "                       padding='pre', truncating='pre')\n",
    "# 预测\n",
    "\n",
    "print(tokens_pad)\n",
    "result = model_loaded.predict(tokens_pad)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
