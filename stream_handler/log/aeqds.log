2020-04-22 18:48:06,494 [main] [org.apache.spark.streaming.StreamingContext] [ERROR] - Error starting the context, marking it as stopped%java.lang.IllegalArgumentException: requirement failed: No output operations registered, so nothing to execute
	at scala.Predef$.require(Predef.scala:277)
	at org.apache.spark.streaming.DStreamGraph.validate(DStreamGraph.scala:168)
	at org.apache.spark.streaming.StreamingContext.validate(StreamingContext.scala:513)
	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:573)
	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)
	at Main$.main(Main.scala:41)
	at Main.main(Main.scala)
2020-04-22 18:48:56,560 [main] [org.apache.spark.SparkContext] [ERROR] - Error initializing SparkContext.%java.lang.IllegalStateException: Shutdown hooks cannot be modified during shutdown.
	at org.apache.spark.util.SparkShutdownHookManager.add(ShutdownHookManager.scala:195)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:572)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:85)
	at Main$.main(Main.scala:16)
	at Main.main(Main.scala)
2020-04-22 18:54:32,109 [Executor task launch worker for task 7] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 7.0 (TID 7)%java.io.NotSerializableException: org.apache.kafka.common.header.internals.RecordHeaders
Serialization stack:
	- object not serializable (class: org.apache.kafka.common.header.internals.RecordHeaders, value: RecordHeaders(headers = [], isReadOnly = false))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.common.header.Headers;, size 2)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 18:54:32,138 [task-result-getter-3] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 7.0 (TID 7) had a not serializable result: org.apache.kafka.common.header.internals.RecordHeaders
Serialization stack:
	- object not serializable (class: org.apache.kafka.common.header.internals.RecordHeaders, value: RecordHeaders(headers = [], isReadOnly = false))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.common.header.Headers;, size 2); not retrying%2020-04-22 18:54:32,150 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587552872000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 7.0 (TID 7) had a not serializable result: org.apache.kafka.common.header.internals.RecordHeaders
Serialization stack:
	- object not serializable (class: org.apache.kafka.common.header.internals.RecordHeaders, value: RecordHeaders(headers = [], isReadOnly = false))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.common.header.Headers;, size 2)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at Main$.$anonfun$main$4(Main.scala:40)
	at Main$.$anonfun$main$4$adapted(Main.scala:39)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 18:54:36,353 [Executor task launch worker for task 0] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 0.0 (TID 0)%java.io.NotSerializableException: org.apache.kafka.common.header.internals.RecordHeaders
Serialization stack:
	- object not serializable (class: org.apache.kafka.common.header.internals.RecordHeaders, value: RecordHeaders(headers = [], isReadOnly = false))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.common.header.Headers;, size 2)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 18:54:36,381 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 0.0 (TID 0) had a not serializable result: org.apache.kafka.common.header.internals.RecordHeaders
Serialization stack:
	- object not serializable (class: org.apache.kafka.common.header.internals.RecordHeaders, value: RecordHeaders(headers = [], isReadOnly = false))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.common.header.Headers;, size 2); not retrying%2020-04-22 18:54:36,395 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587552876000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 0.0 (TID 0) had a not serializable result: org.apache.kafka.common.header.internals.RecordHeaders
Serialization stack:
	- object not serializable (class: org.apache.kafka.common.header.internals.RecordHeaders, value: RecordHeaders(headers = [], isReadOnly = false))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.common.header.Headers;, size 2)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at Main$.$anonfun$main$4(Main.scala:40)
	at Main$.$anonfun$main$4$adapted(Main.scala:39)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 19:48:54,133 [Executor task launch worker for task 1] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 1.0 (TID 1)%java.io.NotSerializableException: org.apache.kafka.common.header.internals.RecordHeaders
Serialization stack:
	- object not serializable (class: org.apache.kafka.common.header.internals.RecordHeaders, value: RecordHeaders(headers = [], isReadOnly = false))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.common.header.Headers;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 19:48:54,163 [task-result-getter-1] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.kafka.common.header.internals.RecordHeaders
Serialization stack:
	- object not serializable (class: org.apache.kafka.common.header.internals.RecordHeaders, value: RecordHeaders(headers = [], isReadOnly = false))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.common.header.Headers;, size 1); not retrying%2020-04-22 19:48:54,172 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587556134000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.kafka.common.header.internals.RecordHeaders
Serialization stack:
	- object not serializable (class: org.apache.kafka.common.header.internals.RecordHeaders, value: RecordHeaders(headers = [], isReadOnly = false))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.common.header.Headers;, size 1)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at Main$.$anonfun$main$4(Main.scala:41)
	at Main$.$anonfun$main$4$adapted(Main.scala:40)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 19:53:06,415 [Executor task launch worker for task 0] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 0.0 (TID 0)%java.io.NotSerializableException: org.apache.kafka.common.header.internals.RecordHeaders
Serialization stack:
	- object not serializable (class: org.apache.kafka.common.header.internals.RecordHeaders, value: RecordHeaders(headers = [], isReadOnly = false))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.common.header.Headers;, size 11)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 19:53:06,439 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 0.0 (TID 0) had a not serializable result: org.apache.kafka.common.header.internals.RecordHeaders
Serialization stack:
	- object not serializable (class: org.apache.kafka.common.header.internals.RecordHeaders, value: RecordHeaders(headers = [], isReadOnly = false))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.common.header.Headers;, size 11); not retrying%2020-04-22 19:53:06,455 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587556386000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 0.0 (TID 0) had a not serializable result: org.apache.kafka.common.header.internals.RecordHeaders
Serialization stack:
	- object not serializable (class: org.apache.kafka.common.header.internals.RecordHeaders, value: RecordHeaders(headers = [], isReadOnly = false))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.common.header.Headers;, size 11)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1364)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 20:10:36,544 [Executor task launch worker for task 0] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 0.0 (TID 0)%java.lang.NullPointerException
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 20:10:36,574 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0 in stage 0.0 failed 1 times; aborting job%2020-04-22 20:10:36,594 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587557436000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NullPointerException
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1364)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.NullPointerException
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	... 3 more
2020-04-22 20:34:48,079 [Executor task launch worker for task 10] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 10.0 (TID 10)%java.lang.NullPointerException
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 20:34:48,116 [task-result-getter-2] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0 in stage 10.0 failed 1 times; aborting job%2020-04-22 20:34:48,129 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587558888000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10, localhost, executor driver): java.lang.NullPointerException
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1364)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.NullPointerException
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	... 3 more
2020-04-22 21:28:24,079 [Executor task launch worker for task 3] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 3.0 (TID 3)%org.apache.avro.AvroRuntimeException: Malformed data. Length is negative: -51
	at org.apache.avro.io.BinaryDecoder.doReadBytes(BinaryDecoder.java:336)
	at org.apache.avro.io.BinaryDecoder.readString(BinaryDecoder.java:263)
	at org.apache.avro.io.BinaryDecoder.readString(BinaryDecoder.java:272)
	at Main$.$anonfun$main$1(Main.scala:45)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 21:28:24,109 [task-result-getter-3] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0 in stage 3.0 failed 1 times; aborting job%2020-04-22 21:28:24,119 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587562104000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): org.apache.avro.AvroRuntimeException: Malformed data. Length is negative: -51
	at org.apache.avro.io.BinaryDecoder.doReadBytes(BinaryDecoder.java:336)
	at org.apache.avro.io.BinaryDecoder.readString(BinaryDecoder.java:263)
	at org.apache.avro.io.BinaryDecoder.readString(BinaryDecoder.java:272)
	at Main$.$anonfun$main$1(Main.scala:45)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1364)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: org.apache.avro.AvroRuntimeException: Malformed data. Length is negative: -51
	at org.apache.avro.io.BinaryDecoder.doReadBytes(BinaryDecoder.java:336)
	at org.apache.avro.io.BinaryDecoder.readString(BinaryDecoder.java:263)
	at org.apache.avro.io.BinaryDecoder.readString(BinaryDecoder.java:272)
	at Main$.$anonfun$main$1(Main.scala:45)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	... 3 more
2020-04-22 21:39:34,935 [Executor task launch worker for task 0] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 0.0 (TID 0)%java.lang.IllegalArgumentException: capacity < 0: (-40349688 < 0)
	at java.base/java.nio.Buffer.createCapacityException(Buffer.java:257)
	at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:350)
	at org.apache.avro.io.BinaryDecoder.readBytes(BinaryDecoder.java:288)
	at org.apache.avro.io.ResolvingDecoder.readBytes(ResolvingDecoder.java:237)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:495)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:488)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:182)
	at org.apache.avro.specific.SpecificDatumReader.readField(SpecificDatumReader.java:116)
	at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:222)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:175)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:145)
	at Main$.$anonfun$main$1(Main.scala:43)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 21:39:34,985 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0 in stage 0.0 failed 1 times; aborting job%2020-04-22 21:39:35,014 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587562774000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.IllegalArgumentException: capacity < 0: (-40349688 < 0)
	at java.base/java.nio.Buffer.createCapacityException(Buffer.java:257)
	at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:350)
	at org.apache.avro.io.BinaryDecoder.readBytes(BinaryDecoder.java:288)
	at org.apache.avro.io.ResolvingDecoder.readBytes(ResolvingDecoder.java:237)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:495)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:488)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:182)
	at org.apache.avro.specific.SpecificDatumReader.readField(SpecificDatumReader.java:116)
	at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:222)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:175)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:145)
	at Main$.$anonfun$main$1(Main.scala:43)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1364)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.IllegalArgumentException: capacity < 0: (-40349688 < 0)
	at java.base/java.nio.Buffer.createCapacityException(Buffer.java:257)
	at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:350)
	at org.apache.avro.io.BinaryDecoder.readBytes(BinaryDecoder.java:288)
	at org.apache.avro.io.ResolvingDecoder.readBytes(ResolvingDecoder.java:237)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:495)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:488)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:182)
	at org.apache.avro.specific.SpecificDatumReader.readField(SpecificDatumReader.java:116)
	at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:222)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:175)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:145)
	at Main$.$anonfun$main$1(Main.scala:43)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	... 3 more
2020-04-22 21:44:24,271 [Executor task launch worker for task 4] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 4.0 (TID 4)%java.lang.IllegalArgumentException: capacity < 0: (-4698104 < 0)
	at java.base/java.nio.Buffer.createCapacityException(Buffer.java:257)
	at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:350)
	at org.apache.avro.io.BinaryDecoder.readBytes(BinaryDecoder.java:288)
	at org.apache.avro.io.ResolvingDecoder.readBytes(ResolvingDecoder.java:237)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:495)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:488)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:182)
	at org.apache.avro.specific.SpecificDatumReader.readField(SpecificDatumReader.java:116)
	at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:222)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:175)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:145)
	at DataParser$.parse(DataParser.scala:12)
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 21:44:24,302 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0 in stage 4.0 failed 1 times; aborting job%2020-04-22 21:44:24,313 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587563064000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): java.lang.IllegalArgumentException: capacity < 0: (-4698104 < 0)
	at java.base/java.nio.Buffer.createCapacityException(Buffer.java:257)
	at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:350)
	at org.apache.avro.io.BinaryDecoder.readBytes(BinaryDecoder.java:288)
	at org.apache.avro.io.ResolvingDecoder.readBytes(ResolvingDecoder.java:237)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:495)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:488)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:182)
	at org.apache.avro.specific.SpecificDatumReader.readField(SpecificDatumReader.java:116)
	at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:222)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:175)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:145)
	at DataParser$.parse(DataParser.scala:12)
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1364)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.IllegalArgumentException: capacity < 0: (-4698104 < 0)
	at java.base/java.nio.Buffer.createCapacityException(Buffer.java:257)
	at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:350)
	at org.apache.avro.io.BinaryDecoder.readBytes(BinaryDecoder.java:288)
	at org.apache.avro.io.ResolvingDecoder.readBytes(ResolvingDecoder.java:237)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:495)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:488)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:182)
	at org.apache.avro.specific.SpecificDatumReader.readField(SpecificDatumReader.java:116)
	at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:222)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:175)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:145)
	at DataParser$.parse(DataParser.scala:12)
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	... 3 more
2020-04-22 21:47:46,622 [Executor task launch worker for task 5] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 5.0 (TID 5)%java.lang.IllegalArgumentException: capacity < 0: (-4698104 < 0)
	at java.base/java.nio.Buffer.createCapacityException(Buffer.java:257)
	at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:350)
	at org.apache.avro.io.BinaryDecoder.readBytes(BinaryDecoder.java:288)
	at org.apache.avro.io.ResolvingDecoder.readBytes(ResolvingDecoder.java:237)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:495)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:488)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:182)
	at org.apache.avro.specific.SpecificDatumReader.readField(SpecificDatumReader.java:116)
	at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:222)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:175)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:145)
	at DataParser$.parse(DataParser.scala:12)
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 21:47:46,653 [task-result-getter-1] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0 in stage 5.0 failed 1 times; aborting job%2020-04-22 21:47:46,667 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587563266000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5, localhost, executor driver): java.lang.IllegalArgumentException: capacity < 0: (-4698104 < 0)
	at java.base/java.nio.Buffer.createCapacityException(Buffer.java:257)
	at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:350)
	at org.apache.avro.io.BinaryDecoder.readBytes(BinaryDecoder.java:288)
	at org.apache.avro.io.ResolvingDecoder.readBytes(ResolvingDecoder.java:237)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:495)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:488)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:182)
	at org.apache.avro.specific.SpecificDatumReader.readField(SpecificDatumReader.java:116)
	at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:222)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:175)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:145)
	at DataParser$.parse(DataParser.scala:12)
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1364)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.IllegalArgumentException: capacity < 0: (-4698104 < 0)
	at java.base/java.nio.Buffer.createCapacityException(Buffer.java:257)
	at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:350)
	at org.apache.avro.io.BinaryDecoder.readBytes(BinaryDecoder.java:288)
	at org.apache.avro.io.ResolvingDecoder.readBytes(ResolvingDecoder.java:237)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:495)
	at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:488)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:182)
	at org.apache.avro.specific.SpecificDatumReader.readField(SpecificDatumReader.java:116)
	at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:222)
	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:175)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:145)
	at DataParser$.parse(DataParser.scala:12)
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	... 3 more
2020-04-22 21:52:59,576 [main] [org.apache.spark.streaming.StreamingContext] [ERROR] - Error starting the context, marking it as stopped%org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:799)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:615)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:596)
	at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:73)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:259)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7(DStreamGraph.scala:54)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7$adapted(DStreamGraph.scala:54)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:142)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:970)
	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:49)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:63)
	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:52)
	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:46)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:967)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:149)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:145)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1016)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1665)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1598)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)
	at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)
	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)
	at Main$.main(Main.scala:56)
	at Main.main(Main.scala)
Caused by: org.apache.kafka.common.KafkaException: Could not find a public no-argument constructor for org.apache.flume.serialization.AvroEventDeserializer
	at org.apache.kafka.common.utils.Utils.newInstance(Utils.java:308)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstance(AbstractConfig.java:302)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:699)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:615)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:596)
	at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:73)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:259)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7(DStreamGraph.scala:54)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7$adapted(DStreamGraph.scala:54)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:142)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:970)
	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:49)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:63)
	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:52)
	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:46)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:967)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:149)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:145)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1016)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1665)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1598)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)
Caused by: java.lang.NoSuchMethodException: org.apache.flume.serialization.AvroEventDeserializer.<init>()
	at java.base/java.lang.Class.getConstructor0(Class.java:3350)
	at java.base/java.lang.Class.getDeclaredConstructor(Class.java:2554)
	at org.apache.kafka.common.utils.Utils.newInstance(Utils.java:306)
	... 26 more
2020-04-22 21:53:30,944 [main] [org.apache.spark.streaming.StreamingContext] [ERROR] - Error starting the context, marking it as stopped%org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:799)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:615)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:596)
	at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:73)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:259)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7(DStreamGraph.scala:54)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7$adapted(DStreamGraph.scala:54)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:142)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:970)
	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:49)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:63)
	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:52)
	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:46)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:967)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:149)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:145)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1016)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1665)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1598)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)
	at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)
	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)
	at Main$.main(Main.scala:56)
	at Main.main(Main.scala)
Caused by: org.apache.kafka.common.KafkaException: Could not find a public no-argument constructor for org.apache.flume.serialization.AvroEventDeserializer
	at org.apache.kafka.common.utils.Utils.newInstance(Utils.java:308)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstance(AbstractConfig.java:302)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:699)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:615)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:596)
	at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:73)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:259)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7(DStreamGraph.scala:54)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7$adapted(DStreamGraph.scala:54)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:142)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:970)
	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:49)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:63)
	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:52)
	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:46)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:967)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:149)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:145)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1016)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1665)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1598)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)
Caused by: java.lang.NoSuchMethodException: org.apache.flume.serialization.AvroEventDeserializer.<init>()
	at java.base/java.lang.Class.getConstructor0(Class.java:3350)
	at java.base/java.lang.Class.getDeclaredConstructor(Class.java:2554)
	at org.apache.kafka.common.utils.Utils.newInstance(Utils.java:306)
	... 26 more
2020-04-22 21:54:46,097 [main] [org.apache.spark.streaming.StreamingContext] [ERROR] - Error starting the context, marking it as stopped%org.apache.kafka.common.config.ConfigException: Invalid value org.apache.flume.serialization.BytesDeserializer for configuration value.deserializer: Class org.apache.flume.serialization.BytesDeserializer could not be found.
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:724)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:469)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:462)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:62)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:75)
	at org.apache.kafka.clients.consumer.ConsumerConfig.<init>(ConsumerConfig.java:499)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:615)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:596)
	at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:73)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:259)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7(DStreamGraph.scala:54)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7$adapted(DStreamGraph.scala:54)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:142)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:970)
	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:49)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:63)
	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:52)
	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:46)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:967)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:149)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:145)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1016)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1665)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1598)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)
	at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)
	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)
	at Main$.main(Main.scala:56)
	at Main.main(Main.scala)
2020-04-22 21:55:06,175 [main] [org.apache.spark.streaming.StreamingContext] [ERROR] - Error starting the context, marking it as stopped%org.apache.kafka.common.config.ConfigException: Invalid value org.apache.flume.serialization.BytesDeserializer for configuration value.deserializer: Class org.apache.flume.serialization.BytesDeserializer could not be found.
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:724)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:469)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:462)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:62)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:75)
	at org.apache.kafka.clients.consumer.ConsumerConfig.<init>(ConsumerConfig.java:499)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:615)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:596)
	at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:73)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:259)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7(DStreamGraph.scala:54)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7$adapted(DStreamGraph.scala:54)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:142)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:970)
	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:49)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:63)
	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:52)
	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:46)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:967)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:149)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:145)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1016)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1665)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1598)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)
	at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)
	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)
	at Main$.main(Main.scala:55)
	at Main.main(Main.scala)
2020-04-22 21:55:32,665 [Executor task launch worker for task 0] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 0.0 (TID 0)%java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class java.lang.String (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 21:55:32,703 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0 in stage 0.0 failed 1 times; aborting job%2020-04-22 21:55:32,723 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587563732000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class java.lang.String (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1364)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class java.lang.String (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:39)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	... 3 more
2020-04-22 21:58:54,316 [Executor task launch worker for task 6] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 6.0 (TID 6)%java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class java.lang.String (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:40)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 21:58:54,350 [task-result-getter-2] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0 in stage 6.0 failed 1 times; aborting job%2020-04-22 21:58:54,360 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587563934000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6, localhost, executor driver): java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class java.lang.String (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:40)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1364)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class java.lang.String (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:40)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	... 3 more
2020-04-22 22:00:40,759 [Executor task launch worker for task 0] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 0.0 (TID 0)%java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class java.lang.String (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:40)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 22:00:40,791 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0 in stage 0.0 failed 1 times; aborting job%2020-04-22 22:00:40,814 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587564040000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class java.lang.String (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:40)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1364)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class java.lang.String (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:40)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	... 3 more
2020-04-22 22:10:30,267 [main] [org.apache.spark.streaming.StreamingContext] [ERROR] - Error starting the context, marking it as stopped%org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:799)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:615)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:596)
	at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:73)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:259)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7(DStreamGraph.scala:54)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7$adapted(DStreamGraph.scala:54)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:142)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:970)
	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:49)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:63)
	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:52)
	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:46)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:967)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:149)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:145)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1016)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1665)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1598)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)
	at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)
	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)
	at Main$.main(Main.scala:56)
	at Main.main(Main.scala)
Caused by: org.apache.kafka.common.KafkaException: org.apache.kafka.common.serialization.ByteArraySerializer is not an instance of org.apache.kafka.common.serialization.Deserializer
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstance(AbstractConfig.java:304)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:699)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:615)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:596)
	at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:73)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:259)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7(DStreamGraph.scala:54)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7$adapted(DStreamGraph.scala:54)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:142)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:970)
	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:49)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:63)
	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:52)
	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:46)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:967)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:149)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:145)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1016)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1665)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1598)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)
2020-04-22 22:12:50,538 [main] [org.apache.spark.streaming.StreamingContext] [ERROR] - Error starting the context, marking it as stopped%org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:799)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:615)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:596)
	at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:73)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:259)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7(DStreamGraph.scala:54)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7$adapted(DStreamGraph.scala:54)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:142)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:970)
	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:49)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:63)
	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:52)
	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:46)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:967)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:149)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:145)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1016)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1665)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1598)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)
	at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)
	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)
	at Main$.main(Main.scala:56)
	at Main.main(Main.scala)
Caused by: org.apache.kafka.common.KafkaException: org.apache.kafka.common.serialization.ByteArraySerializer is not an instance of org.apache.kafka.common.serialization.Deserializer
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstance(AbstractConfig.java:304)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:699)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:615)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:596)
	at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:84)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:73)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:259)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7(DStreamGraph.scala:54)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$start$7$adapted(DStreamGraph.scala:54)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:142)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:970)
	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:49)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:63)
	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:52)
	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:46)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:967)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:149)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:145)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1016)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1665)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1598)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)
2020-04-22 22:13:34,574 [Executor task launch worker for task 0] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 0.0 (TID 0)%java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class java.lang.String (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:40)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 22:13:34,604 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0 in stage 0.0 failed 1 times; aborting job%2020-04-22 22:13:34,628 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587564814000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class java.lang.String (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:40)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1364)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class java.lang.String (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:40)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	... 3 more
2020-04-22 22:18:44,309 [Executor task launch worker for task 8] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 8.0 (TID 8)%java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class [B (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; [B is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:43)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-22 22:18:44,345 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0 in stage 8.0 failed 1 times; aborting job%2020-04-22 22:18:44,357 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587565124000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8, localhost, executor driver): java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class [B (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; [B is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:43)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1364)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.ClassCastException: class org.apache.kafka.common.utils.Bytes cannot be cast to class [B (org.apache.kafka.common.utils.Bytes is in unnamed module of loader 'app'; [B is in module java.base of loader 'bootstrap')
	at Main$.$anonfun$main$1(Main.scala:43)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
	at scala.collection.Iterator$SliceIterator.next(Iterator.scala:259)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1406)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1364)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	... 3 more
2020-04-23 17:52:53,476 [Executor task launch worker for task 172] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-23 17:52:53,480 [Executor task launch worker for task 172] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-23 17:52:55,349 [Executor task launch worker for task 172] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 1.0 in stage 57.0 (TID 172)%org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family answers does not exist in region zhihu:words_index,,1587635268971.6e6a5e60f07d7d7767f9c2531807b1e1. in table 'zhihu:words_index', {NAME => 'answer_content', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}, {NAME => 'question_title', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.doBatchOp(RSRpcServices.java:890)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.doNonAtomicRegionMutation(RSRpcServices.java:832)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.multi(RSRpcServices.java:2368)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:35271)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2395)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:123)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:188)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:168)
: 1 time, servers with issues: thanos.localdomain,16201,1587433527096
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:260)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$2400(AsyncProcess.java:240)
	at org.apache.hadoop.hbase.client.AsyncProcess.waitForAllPreviousOpsAndReset(AsyncProcess.java:1867)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:247)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.flush(BufferedMutatorImpl.java:197)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:1461)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:1017)
	at utils.HBaseUtil$.insertData(HBaseUtil.scala:84)
	at services.Persist$.$anonfun$insertAnswerContentWordSegIndex$1(Persist.scala:19)
	at services.Persist$.$anonfun$insertAnswerContentWordSegIndex$1$adapted(Persist.scala:15)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:32)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:29)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:193)
	at services.Persist$.insertAnswerContentWordSegIndex(Persist.scala:15)
	at services.Persist$.persistAnswer(Persist.scala:60)
	at Main$.$anonfun$main$9(Main.scala:49)
	at Main$.$anonfun$main$9$adapted(Main.scala:48)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:927)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:927)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-23 17:52:55,466 [task-result-getter-3] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 1 in stage 57.0 failed 1 times; aborting job%2020-04-23 17:52:55,486 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587635572000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 57.0 failed 1 times, most recent failure: Lost task 1.0 in stage 57.0 (TID 172, localhost, executor driver): org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family answers does not exist in region zhihu:words_index,,1587635268971.6e6a5e60f07d7d7767f9c2531807b1e1. in table 'zhihu:words_index', {NAME => 'answer_content', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}, {NAME => 'question_title', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.doBatchOp(RSRpcServices.java:890)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.doNonAtomicRegionMutation(RSRpcServices.java:832)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.multi(RSRpcServices.java:2368)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:35271)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2395)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:123)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:188)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:168)
: 1 time, servers with issues: thanos.localdomain,16201,1587433527096
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:260)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$2400(AsyncProcess.java:240)
	at org.apache.hadoop.hbase.client.AsyncProcess.waitForAllPreviousOpsAndReset(AsyncProcess.java:1867)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:247)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.flush(BufferedMutatorImpl.java:197)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:1461)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:1017)
	at utils.HBaseUtil$.insertData(HBaseUtil.scala:84)
	at services.Persist$.$anonfun$insertAnswerContentWordSegIndex$1(Persist.scala:19)
	at services.Persist$.$anonfun$insertAnswerContentWordSegIndex$1$adapted(Persist.scala:15)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:32)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:29)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:193)
	at services.Persist$.insertAnswerContentWordSegIndex(Persist.scala:15)
	at services.Persist$.persistAnswer(Persist.scala:60)
	at Main$.$anonfun$main$9(Main.scala:49)
	at Main$.$anonfun$main$9$adapted(Main.scala:48)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:927)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:927)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:927)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.foreach(RDD.scala:925)
	at Main$.$anonfun$main$8(Main.scala:48)
	at Main$.$anonfun$main$8$adapted(Main.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-23 17:52:56,142 [Executor task launch worker for task 182] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 2.0 in stage 60.0 (TID 182)%org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family answers does not exist in region zhihu:words_index,,1587635268971.6e6a5e60f07d7d7767f9c2531807b1e1. in table 'zhihu:words_index', {NAME => 'answer_content', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}, {NAME => 'question_title', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.doBatchOp(RSRpcServices.java:890)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.doNonAtomicRegionMutation(RSRpcServices.java:832)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.multi(RSRpcServices.java:2368)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:35271)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2395)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:123)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:188)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:168)
: 1 time, servers with issues: thanos.localdomain,16201,1587433527096
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:260)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$2400(AsyncProcess.java:240)
	at org.apache.hadoop.hbase.client.AsyncProcess.waitForAllPreviousOpsAndReset(AsyncProcess.java:1867)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:247)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.flush(BufferedMutatorImpl.java:197)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:1461)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:1017)
	at utils.HBaseUtil$.insertData(HBaseUtil.scala:84)
	at services.Persist$.$anonfun$insertAnswerContentWordSegIndex$1(Persist.scala:19)
	at services.Persist$.$anonfun$insertAnswerContentWordSegIndex$1$adapted(Persist.scala:15)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:32)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:29)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:193)
	at services.Persist$.insertAnswerContentWordSegIndex(Persist.scala:15)
	at services.Persist$.persistAnswer(Persist.scala:60)
	at Main$.$anonfun$main$9(Main.scala:49)
	at Main$.$anonfun$main$9$adapted(Main.scala:48)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:927)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:927)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-23 17:59:45,254 [Executor task launch worker for task 38] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-23 17:59:45,261 [Executor task launch worker for task 38] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-23 17:59:49,010 [Executor task launch worker for task 38] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 2.0 in stage 12.0 (TID 38)%java.lang.StringIndexOutOfBoundsException: begin 0, end -1, length 0
	at java.base/java.lang.String.checkBoundsBeginEnd(String.java:3720)
	at java.base/java.lang.String.substring(String.java:1909)
	at services.Persist$.$anonfun$persistWordsSegment$1(Persist.scala:48)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:32)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:29)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:193)
	at scala.collection.TraversableLike.map(TraversableLike.scala:234)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:227)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:193)
	at services.Persist$.persistWordsSegment(Persist.scala:42)
	at services.Persist$.persistAnswer(Persist.scala:61)
	at Main$.$anonfun$main$9(Main.scala:49)
	at Main$.$anonfun$main$9$adapted(Main.scala:48)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:927)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:927)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-23 17:59:49,528 [task-result-getter-3] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 2 in stage 12.0 failed 1 times; aborting job%2020-04-23 17:59:49,656 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587635984000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 12.0 failed 1 times, most recent failure: Lost task 2.0 in stage 12.0 (TID 38, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: begin 0, end -1, length 0
	at java.base/java.lang.String.checkBoundsBeginEnd(String.java:3720)
	at java.base/java.lang.String.substring(String.java:1909)
	at services.Persist$.$anonfun$persistWordsSegment$1(Persist.scala:48)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:32)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:29)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:193)
	at scala.collection.TraversableLike.map(TraversableLike.scala:234)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:227)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:193)
	at services.Persist$.persistWordsSegment(Persist.scala:42)
	at services.Persist$.persistAnswer(Persist.scala:61)
	at Main$.$anonfun$main$9(Main.scala:49)
	at Main$.$anonfun$main$9$adapted(Main.scala:48)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:927)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:927)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:927)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.foreach(RDD.scala:925)
	at Main$.$anonfun$main$8(Main.scala:48)
	at Main$.$anonfun$main$8$adapted(Main.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.StringIndexOutOfBoundsException: begin 0, end -1, length 0
	at java.base/java.lang.String.checkBoundsBeginEnd(String.java:3720)
	at java.base/java.lang.String.substring(String.java:1909)
	at services.Persist$.$anonfun$persistWordsSegment$1(Persist.scala:48)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:32)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:29)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:193)
	at scala.collection.TraversableLike.map(TraversableLike.scala:234)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:227)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:193)
	at services.Persist$.persistWordsSegment(Persist.scala:42)
	at services.Persist$.persistAnswer(Persist.scala:61)
	at Main$.$anonfun$main$9(Main.scala:49)
	at Main$.$anonfun$main$9$adapted(Main.scala:48)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:927)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:927)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	... 3 more
2020-04-23 17:59:50,295 [Executor task launch worker for task 39] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 13.0 (TID 39)%java.lang.StringIndexOutOfBoundsException: begin 0, end -1, length 0
	at java.base/java.lang.String.checkBoundsBeginEnd(String.java:3720)
	at java.base/java.lang.String.substring(String.java:1909)
	at services.Persist$.$anonfun$persistWordsSegment$1(Persist.scala:48)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:32)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:29)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:193)
	at scala.collection.TraversableLike.map(TraversableLike.scala:234)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:227)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:193)
	at services.Persist$.persistWordsSegment(Persist.scala:42)
	at services.Persist$.persistUser(Persist.scala:74)
	at Main$.$anonfun$main$11(Main.scala:54)
	at Main$.$anonfun$main$11$adapted(Main.scala:53)
	at scala.collection.Iterator.foreach(Iterator.scala:929)
	at scala.collection.Iterator.foreach$(Iterator.scala:929)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1406)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:927)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:927)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-23 18:02:25,704 [Executor task launch worker for task 2] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-23 18:02:25,708 [Executor task launch worker for task 2] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-23 18:05:09,046 [dispatcher-event-loop-0] [org.apache.spark.scheduler.TaskSchedulerImpl] [ERROR] - Lost executor driver on localhost: Executor heartbeat timed out after 125305 ms%2020-04-23 18:05:10,061 [JobGenerator] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] [ERROR] - [Consumer clientId=consumer-1, groupId=group1] Offset commit failed on partition zhihu_questions-0 at offset 659: The coordinator is not aware of this member.%2020-04-23 18:05:10,796 [dispatcher-event-loop-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 2 in stage 0.0 failed 1 times; aborting job%2020-04-23 18:05:11,851 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587636144000 ms.0%org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 125305 ms
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:927)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.foreach(RDD.scala:925)
	at Main$.$anonfun$main$8(Main.scala:48)
	at Main$.$anonfun$main$8$adapted(Main.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-23 18:05:15,840 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error generating jobs for time 1587636244000 ms%java.lang.IllegalStateException: This consumer has already been closed.
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2202)
	at org.apache.kafka.clients.consumer.KafkaConsumer.assignment(KafkaConsumer.java:847)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.paranoidPoll(DirectKafkaInputDStream.scala:171)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.latestOffsets(DirectKafkaInputDStream.scala:191)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:228)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:341)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:334)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:341)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:334)
	at org.apache.spark.streaming.dstream.FilteredDStream.compute(FilteredDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:341)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:334)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:341)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:334)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:122)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:241)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:238)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
	at org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:249)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2020-04-23 18:09:19,438 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-23 18:09:19,443 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-23 18:23:26,772 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-23 18:23:26,775 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-23 18:26:17,178 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-23 18:26:17,181 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-23 18:29:41,666 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-23 18:29:41,669 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-23 18:43:47,219 [Executor task launch worker for task 65] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-23 18:43:47,223 [Executor task launch worker for task 65] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-23 18:52:33,370 [Executor task launch worker for task 64] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-23 18:52:33,374 [Executor task launch worker for task 64] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 12:31:48,503 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587788992000 ms.2%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$17(Main.scala:62)
	at Main$.$anonfun$main$17$adapted(Main.scala:59)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 12:33:40,770 [Executor task launch worker for task 1] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 12:33:40,772 [Executor task launch worker for task 1] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 12:35:20,749 [Executor task launch worker for task 1] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 12:35:20,752 [Executor task launch worker for task 1] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 12:36:49,305 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587789320000 ms.2%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$17(Main.scala:71)
	at Main$.$anonfun$main$17$adapted(Main.scala:68)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 12:37:20,533 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587789420000 ms.2%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$17(Main.scala:77)
	at Main$.$anonfun$main$17$adapted(Main.scala:71)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 12:38:17,288 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587789450000 ms.2%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$17(Main.scala:77)
	at Main$.$anonfun$main$17$adapted(Main.scala:71)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 12:38:34,727 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587789506000 ms.2%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$17(Main.scala:77)
	at Main$.$anonfun$main$17$adapted(Main.scala:71)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 12:38:48,490 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587789524000 ms.2%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$17(Main.scala:77)
	at Main$.$anonfun$main$17$adapted(Main.scala:71)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 13:18:30,482 [Executor task launch worker for task 11] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 13:18:30,485 [Executor task launch worker for task 11] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 13:18:40,821 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587791910000 ms.0%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionTotalAnswersCount(Redis.scala:86)
	at services.Redis$.$anonfun$appendQuestionTotalAnswersCount$1(Redis.scala:82)
	at services.Redis$.$anonfun$appendQuestionTotalAnswersCount$1$adapted(Redis.scala:82)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:32)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:29)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:193)
	at services.Redis$.appendQuestionTotalAnswersCount(Redis.scala:82)
	at Main$.$anonfun$main$8(Main.scala:55)
	at Main$.$anonfun$main$8$adapted(Main.scala:43)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 26 more
2020-04-25 13:27:25,167 [Executor task launch worker for task 1] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 13:27:25,174 [Executor task launch worker for task 1] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 13:58:10,450 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587794236000 ms.2%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$17(Main.scala:67)
	at Main$.$anonfun$main$17$adapted(Main.scala:63)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 13:58:18,834 [Executor task launch worker for task 2] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 13:58:18,837 [Executor task launch worker for task 2] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 13:59:59,256 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587794364000 ms.2%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$17(Main.scala:67)
	at Main$.$anonfun$main$17$adapted(Main.scala:63)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 14:05:34,136 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587794654000 ms.2%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$17(Main.scala:67)
	at Main$.$anonfun$main$17$adapted(Main.scala:63)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 14:06:19,177 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587794744000 ms.1%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$15(Main.scala:60)
	at Main$.$anonfun$main$15$adapted(Main.scala:56)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 14:09:38,632 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587794964000 ms.1%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$15(Main.scala:63)
	at Main$.$anonfun$main$15$adapted(Main.scala:59)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 14:11:03,621 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587794992000 ms.1%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$15(Main.scala:63)
	at Main$.$anonfun$main$15$adapted(Main.scala:59)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 14:11:21,776 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587795074000 ms.1%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$15(Main.scala:63)
	at Main$.$anonfun$main$15$adapted(Main.scala:59)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 14:13:17,681 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587795164000 ms.1%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.appendQuestionsNum(Redis.scala:42)
	at Main$.$anonfun$main$15(Main.scala:60)
	at Main$.$anonfun$main$15$adapted(Main.scala:56)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 20 more
2020-04-25 14:18:44,935 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587795504000 ms.1%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.publishTotalQuestionsNum(Redis.scala:70)
	at services.Redis$.appendQuestionsNum(Redis.scala:65)
	at Main$.$anonfun$main$15(Main.scala:60)
	at Main$.$anonfun$main$15$adapted(Main.scala:56)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 21 more
2020-04-25 14:21:30,720 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587795684000 ms.1%redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
	at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
	at services.Redis$.publishTotalQuestionsNum(Redis.scala:61)
	at services.Redis$.appendQuestionsNum(Redis.scala:55)
	at Main$.$anonfun$main$15(Main.scala:60)
	at Main$.$anonfun$main$15$adapted(Main.scala:56)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:590)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:432)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:349)
	at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
	... 21 more
2020-04-25 14:24:56,881 [Executor task launch worker for task 2] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 14:24:56,884 [Executor task launch worker for task 2] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 14:24:59,055 [Executor task launch worker for task 9] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 3.0 (TID 9)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135368, CreateTime = 1587795889513, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@2f1df00a))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 11)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:24:59,079 [task-result-getter-1] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 3.0 (TID 9) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135368, CreateTime = 1587795889513, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@2f1df00a))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 11); not retrying%2020-04-25 14:24:59,086 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587795896000 ms.3%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 3.0 (TID 9) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135368, CreateTime = 1587795889513, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@2f1df00a))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 11)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:139)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:26:24,771 [Executor task launch worker for task 2] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 14:26:24,773 [Executor task launch worker for task 2] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 14:26:27,814 [Executor task launch worker for task 9] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 3.0 (TID 9)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1811, CreateTime = 1587795898513, serialized key size = -1, serialized value size = 259, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@553bc0c7))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 11)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:26:27,833 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 3.0 (TID 9) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1811, CreateTime = 1587795898513, serialized key size = -1, serialized value size = 259, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@553bc0c7))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 11); not retrying%2020-04-25 14:26:27,842 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587795984000 ms.3%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 3.0 (TID 9) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1811, CreateTime = 1587795898513, serialized key size = -1, serialized value size = 259, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@553bc0c7))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 11)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:139)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:27:24,755 [Executor task launch worker for task 1] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 14:27:24,758 [Executor task launch worker for task 1] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 14:27:27,444 [Executor task launch worker for task 9] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 3.0 (TID 9)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135421, CreateTime = 1587795991390, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@79d1c0e0))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 7)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:27:27,444 [Executor task launch worker for task 10] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 1.0 in stage 3.0 (TID 10)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 179993, CreateTime = 1587795991390, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@7b360733))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 4)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:27:27,464 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 3.0 (TID 9) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135421, CreateTime = 1587795991390, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@79d1c0e0))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 7); not retrying%2020-04-25 14:27:27,466 [task-result-getter-2] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 1.0 in stage 3.0 (TID 10) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 179993, CreateTime = 1587795991390, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@7b360733))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 4); not retrying%2020-04-25 14:27:27,472 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587796044000 ms.3%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 3.0 (TID 9) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135421, CreateTime = 1587795991390, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@79d1c0e0))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 7)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:139)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:28:02,408 [Executor task launch worker for task 11] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 14:28:02,410 [Executor task launch worker for task 11] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 14:28:04,558 [Executor task launch worker for task 18] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 6.0 (TID 18)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1858, CreateTime = 1587796081543, serialized key size = -1, serialized value size = 232, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@15ad5364))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:28:04,558 [Executor task launch worker for task 20] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 2.0 in stage 6.0 (TID 20)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180000, CreateTime = 1587796081545, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@6e229ea7))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:28:04,558 [Executor task launch worker for task 19] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 1.0 in stage 6.0 (TID 19)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135428, CreateTime = 1587796081543, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@12f68bbe))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:28:04,579 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 6.0 (TID 18) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1858, CreateTime = 1587796081543, serialized key size = -1, serialized value size = 232, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@15ad5364))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying%2020-04-25 14:28:04,580 [task-result-getter-1] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 1.0 in stage 6.0 (TID 19) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135428, CreateTime = 1587796081543, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@12f68bbe))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying%2020-04-25 14:28:04,581 [task-result-getter-3] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 2.0 in stage 6.0 (TID 20) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180000, CreateTime = 1587796081545, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@6e229ea7))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying%2020-04-25 14:28:04,586 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587796082000 ms.3%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 6.0 (TID 18) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1858, CreateTime = 1587796081543, serialized key size = -1, serialized value size = 232, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@15ad5364))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:139)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:28:35,613 [Executor task launch worker for task 7] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 1.0 in stage 2.0 (TID 7)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135429, CreateTime = 1587796084543, serialized key size = -1, serialized value size = 360, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@8a60c94))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:28:35,613 [Executor task launch worker for task 6] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 2.0 (TID 6)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180001, CreateTime = 1587796084545, serialized key size = -1, serialized value size = 2561, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@5cfe2873))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 10)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:28:35,633 [task-result-getter-2] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 2.0 (TID 6) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180001, CreateTime = 1587796084545, serialized key size = -1, serialized value size = 2561, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@5cfe2873))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 10); not retrying%2020-04-25 14:28:35,635 [task-result-getter-3] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 1.0 in stage 2.0 (TID 7) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135429, CreateTime = 1587796084543, serialized key size = -1, serialized value size = 360, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@8a60c94))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying%2020-04-25 14:28:35,642 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587796114000 ms.3%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 2.0 (TID 6) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180001, CreateTime = 1587796084545, serialized key size = -1, serialized value size = 2561, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@5cfe2873))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 10)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:139)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:29:17,026 [Executor task launch worker for task 0] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 14:29:17,031 [Executor task launch worker for task 0] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 14:29:19,488 [Executor task launch worker for task 8] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 2.0 in stage 2.0 (TID 8)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1869, CreateTime = 1587796116548, serialized key size = -1, serialized value size = 232, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@34d2b876))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:29:19,488 [Executor task launch worker for task 6] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 2.0 (TID 6)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180011, CreateTime = 1587796116550, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@550060c0))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 5)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:29:19,488 [Executor task launch worker for task 7] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 1.0 in stage 2.0 (TID 7)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135439, CreateTime = 1587796116548, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@293abf58))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 5)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:29:19,659 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 1.0 in stage 2.0 (TID 7) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135439, CreateTime = 1587796116548, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@293abf58))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 5); not retrying%2020-04-25 14:29:19,670 [task-result-getter-3] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 2.0 in stage 2.0 (TID 8) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1869, CreateTime = 1587796116548, serialized key size = -1, serialized value size = 232, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@34d2b876))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying%2020-04-25 14:29:19,671 [task-result-getter-2] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 2.0 (TID 6) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180011, CreateTime = 1587796116550, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@550060c0))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 5); not retrying%2020-04-25 14:29:19,676 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587796156000 ms.2%org.apache.spark.SparkException: Job aborted due to stage failure: Task 1.0 in stage 2.0 (TID 7) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135439, CreateTime = 1587796116548, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@293abf58))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 5)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:139)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:29:20,370 [Executor task launch worker for task 17] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 2.0 in stage 5.0 (TID 17)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135444, CreateTime = 1587796157554, serialized key size = -1, serialized value size = 360, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@4b40c12e))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:29:20,371 [Executor task launch worker for task 15] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 5.0 (TID 15)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180016, CreateTime = 1587796157555, serialized key size = -1, serialized value size = 2561, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@65fd23f5))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 5)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:29:20,559 [Executor task launch worker for task 16] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 1.0 in stage 5.0 (TID 16)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1874, CreateTime = 1587796157553, serialized key size = -1, serialized value size = 244, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@35200ab1))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 5)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:37:06,987 [Executor task launch worker for task 0] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 14:37:06,991 [Executor task launch worker for task 0] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 14:37:09,636 [Executor task launch worker for task 10] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 1.0 in stage 3.0 (TID 10)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135449, CreateTime = 1587796160567, serialized key size = -1, serialized value size = 344, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@204e943f))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 3)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:37:09,636 [Executor task launch worker for task 9] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 3.0 (TID 9)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180021, CreateTime = 1587796160555, serialized key size = -1, serialized value size = 283, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@2c576305))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 8)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:37:09,654 [task-result-getter-1] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 3.0 (TID 9) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180021, CreateTime = 1587796160555, serialized key size = -1, serialized value size = 283, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@2c576305))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 8); not retrying%2020-04-25 14:37:09,655 [task-result-getter-2] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 1.0 in stage 3.0 (TID 10) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135449, CreateTime = 1587796160567, serialized key size = -1, serialized value size = 344, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@204e943f))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 3); not retrying%2020-04-25 14:37:09,662 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587796626000 ms.3%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 3.0 (TID 9) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180021, CreateTime = 1587796160555, serialized key size = -1, serialized value size = 283, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@2c576305))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 8)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:139)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:38:00,377 [Executor task launch worker for task 45] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 14:38:00,380 [Executor task launch worker for task 45] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 14:38:02,649 [Executor task launch worker for task 55] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 1.0 in stage 18.0 (TID 55)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1887, CreateTime = 1587796679636, serialized key size = -1, serialized value size = 232, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@1d6b55d9))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:38:02,649 [Executor task launch worker for task 54] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 18.0 (TID 54)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180029, CreateTime = 1587796679635, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@69a7fe27))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:38:02,655 [Executor task launch worker for task 56] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 2.0 in stage 18.0 (TID 56)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135457, CreateTime = 1587796679650, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@bbaf45a))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:38:02,675 [task-result-getter-1] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 1.0 in stage 18.0 (TID 55) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1887, CreateTime = 1587796679636, serialized key size = -1, serialized value size = 232, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@1d6b55d9))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying%2020-04-25 14:38:02,677 [task-result-getter-3] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 2.0 in stage 18.0 (TID 56) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135457, CreateTime = 1587796679650, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@bbaf45a))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying%2020-04-25 14:38:02,678 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 18.0 (TID 54) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180029, CreateTime = 1587796679635, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@69a7fe27))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying%2020-04-25 14:38:02,684 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587796680000 ms.3%org.apache.spark.SparkException: Job aborted due to stage failure: Task 1.0 in stage 18.0 (TID 55) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1887, CreateTime = 1587796679636, serialized key size = -1, serialized value size = 232, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@1d6b55d9))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:139)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:45:33,112 [Executor task launch worker for task 3] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 1.0 (TID 3)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135458, CreateTime = 1587796682650, serialized key size = -1, serialized value size = 360, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@298c2522))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 10)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:45:33,116 [Executor task launch worker for task 4] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 1.0 in stage 1.0 (TID 4)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180030, CreateTime = 1587796682635, serialized key size = -1, serialized value size = 2561, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@6b843164))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:45:33,135 [task-result-getter-3] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 1.0 (TID 3) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135458, CreateTime = 1587796682650, serialized key size = -1, serialized value size = 360, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@298c2522))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 10); not retrying%2020-04-25 14:45:33,136 [task-result-getter-0] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 1.0 in stage 1.0 (TID 4) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180030, CreateTime = 1587796682635, serialized key size = -1, serialized value size = 2561, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@6b843164))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying%2020-04-25 14:45:33,144 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587797132000 ms.1%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 3) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135458, CreateTime = 1587796682650, serialized key size = -1, serialized value size = 360, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@298c2522))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 10)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:139)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:45:49,661 [Executor task launch worker for task 3] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 1.0 (TID 3)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180040, CreateTime = 1587797135448, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@76a4b32d))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 11)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:45:49,680 [task-result-getter-3] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 1.0 (TID 3) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180040, CreateTime = 1587797135448, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@76a4b32d))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 11); not retrying%2020-04-25 14:45:49,689 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587797148000 ms.1%org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 3) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180040, CreateTime = 1587797135448, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@76a4b32d))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 11)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:139)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:47:12,924 [Executor task launch worker for task 3] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 1.0 (TID 3)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135499, CreateTime = 1587797150741, serialized key size = -1, serialized value size = 372, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@63bf9ef1))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 8)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:47:12,924 [Executor task launch worker for task 4] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 1.0 in stage 1.0 (TID 4)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1929, CreateTime = 1587797150717, serialized key size = -1, serialized value size = 212, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@5000fd6a))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 3)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:47:12,948 [task-result-getter-3] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 1.0 in stage 1.0 (TID 4) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1929, CreateTime = 1587797150717, serialized key size = -1, serialized value size = 212, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@5000fd6a))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 3); not retrying%2020-04-25 14:47:12,949 [task-result-getter-1] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 1.0 (TID 3) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135499, CreateTime = 1587797150741, serialized key size = -1, serialized value size = 372, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@63bf9ef1))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 8); not retrying%2020-04-25 14:47:12,956 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587797232000 ms.1%org.apache.spark.SparkException: Job aborted due to stage failure: Task 1.0 in stage 1.0 (TID 4) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1929, CreateTime = 1587797150717, serialized key size = -1, serialized value size = 212, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@5000fd6a))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 3)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:139)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:48:46,597 [Executor task launch worker for task 15] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 0.0 in stage 5.0 (TID 15)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180079, CreateTime = 1587797324743, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@2f36fe9c))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:48:46,596 [Executor task launch worker for task 16] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 1.0 in stage 5.0 (TID 16)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1937, CreateTime = 1587797324743, serialized key size = -1, serialized value size = 232, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@76646509))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:48:46,596 [Executor task launch worker for task 17] [org.apache.spark.executor.Executor] [ERROR] - Exception in task 2.0 in stage 5.0 (TID 17)%java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135507, CreateTime = 1587797324768, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@13330bb1))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:450)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:48:46,625 [task-result-getter-3] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 1.0 in stage 5.0 (TID 16) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1937, CreateTime = 1587797324743, serialized key size = -1, serialized value size = 232, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@76646509))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying%2020-04-25 14:48:46,627 [task-result-getter-2] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 0.0 in stage 5.0 (TID 15) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_answers, partition = 0, offset = 180079, CreateTime = 1587797324743, serialized key size = -1, serialized value size = 314, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@2f36fe9c))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying%2020-04-25 14:48:46,628 [task-result-getter-1] [org.apache.spark.scheduler.TaskSetManager] [ERROR] - Task 2.0 in stage 5.0 (TID 17) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_users, partition = 0, offset = 135507, CreateTime = 1587797324768, serialized key size = -1, serialized value size = 365, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@13330bb1))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying%2020-04-25 14:48:46,635 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587797326000 ms.1%org.apache.spark.SparkException: Job aborted due to stage failure: Task 1.0 in stage 5.0 (TID 16) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord
Serialization stack:
	- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = zhihu_questions, partition = 0, offset = 1937, CreateTime = 1587797324743, serialized key size = -1, serialized value size = 232, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@76646509))
	- element of array (index: 0)
	- array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:139)
	at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:48)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3(DStream.scala:735)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$print$3$adapted(DStream.scala:734)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 14:52:33,559 [Executor task launch worker for task 1] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 14:52:33,566 [Executor task launch worker for task 1] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 15:27:18,615 [JobScheduler] [org.apache.spark.streaming.scheduler.JobScheduler] [ERROR] - Error running job streaming job 1587799638000 ms.0%redis.clients.jedis.exceptions.JedisDataException: Cannot use Jedis when in Multi. Please use Transaction or reset jedis state.
	at redis.clients.jedis.BinaryJedis.checkIsInMultiOrPipeline(BinaryJedis.java:1885)
	at redis.clients.jedis.Jedis.exists(Jedis.java:207)
	at services.Redis$.appendAnswersNum(Redis.scala:21)
	at Main$.$anonfun$main$8(Main.scala:48)
	at Main$.$anonfun$main$8$adapted(Main.scala:43)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:830)
2020-04-25 15:27:52,594 [Executor task launch worker for task 38] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 15:27:52,598 [Executor task launch worker for task 38] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-25 17:23:06,350 [Executor task launch worker for task 28] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-25 17:23:06,353 [Executor task launch worker for task 28] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-26 10:06:57,847 [JobGenerator] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] [ERROR] - [Consumer clientId=consumer-1, groupId=group1] Offset commit failed on partition zhihu_questions-0 at offset 2233: The coordinator is not aware of this member.%2020-04-26 10:09:37,891 [JobGenerator] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] [ERROR] - [Consumer clientId=consumer-1, groupId=group1] Offset commit failed on partition zhihu_questions-0 at offset 2233: The coordinator is not aware of this member.%2020-04-26 22:19:55,271 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-26 22:19:55,273 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-26 22:22:56,955 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-26 22:22:56,957 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 08:50:25,815 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 08:50:25,819 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 10:09:35,472 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 10:09:35,474 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 10:14:41,255 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 10:14:41,257 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 10:20:14,097 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 10:20:14,099 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 10:21:43,253 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 10:21:43,255 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 10:26:34,786 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 10:26:34,789 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 10:42:06,690 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 10:42:06,716 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 20:10:10,646 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 20:10:10,649 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 20:10:56,173 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 20:10:56,174 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 20:12:44,821 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 20:12:44,823 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 20:34:53,885 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 20:34:53,887 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 21:07:33,980 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 21:07:33,982 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 21:11:52,893 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 21:11:52,896 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 21:14:09,719 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 21:14:09,721 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-04-27 21:17:03,805 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-04-27 21:17:03,807 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-05-23 23:45:59,681 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-05-23 23:45:59,684 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-05-23 23:47:36,668 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-05-23 23:47:36,670 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-05-23 23:48:46,418 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-05-23 23:48:46,421 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-05-23 23:49:31,770 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-05-23 23:49:31,773 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-05-23 23:50:07,644 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-05-23 23:50:07,647 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-05-24 13:19:04,751 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-05-24 13:19:04,754 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-05-24 16:42:20,517 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-05-24 16:42:20,519 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-05-24 16:45:30,424 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-05-24 16:45:30,426 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-05-25 11:12:00,544 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-05-25 11:12:00,546 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-05-25 21:05:59,294 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-05-25 21:05:59,298 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-05-25 21:06:34,344 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-05-25 21:06:34,346 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%2020-05-25 21:10:48,123 [main] [org.ansj.library.AmbiguityLibrary] [ERROR] - Init ambiguity library error :org.ansj.exception.LibraryException:  path :library/ambiguity.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/ambiguity.dic not found or can not to read, path: library/ambiguity.dic%2020-05-25 21:10:48,126 [main] [org.ansj.library.DicLibrary] [ERROR] - Init dic library error :org.ansj.exception.LibraryException:  path :library/default.dic file:/home/xuranus/IdeaProjects/zhihu_realtime/library/default.dic not found or can not to read, path: library/default.dic%